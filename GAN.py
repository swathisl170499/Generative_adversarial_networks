import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# Step 1: Create a more realistic synthetic dataset (simulating real-world customer data)
def generate_realistic_data(num_samples):
    customer_ids = np.arange(1, num_samples + 1)  # Unique Customer IDs
    income = np.random.normal(60, 20, num_samples).clip(20, 150)  # Income in thousands
    loyalty_tier = np.random.choice([1, 2, 3], num_samples,
                                    p=[0.5, 0.3, 0.2])  # Loyalty tier: 1 (basic), 2 (silver), 3 (gold)

    # Spending score and transactions influenced by loyalty tier and income
    spending_score = (income / 2) + (loyalty_tier * 10) + np.random.normal(0, 5, num_samples)
    spending_score = np.clip(spending_score, 1, 100)  # Keep spending score within [1, 100]

    transactions = (income / 2) + (loyalty_tier * 5) + np.random.normal(0, 5, num_samples)
    transactions = np.clip(transactions, 1, 100)  # Keep transactions reasonable

    # Create a DataFrame to simulate customer data
    df = pd.DataFrame({
        'Customer_ID': customer_ids,
        'Income': income,
        'Loyalty Tier': loyalty_tier,
        'Spending Score': spending_score,
        'Transactions': transactions
    })

    return df


# Step 2: Generate realistic customer data for 5000 samples
realistic_data = generate_realistic_data(5000)

# Normalize selected features for GAN training
features_to_normalize = ['Income', 'Spending Score', 'Transactions']
normalized_data = realistic_data[features_to_normalize].apply(lambda x: (x - x.mean()) / x.std())
real_data_tensor = torch.FloatTensor(normalized_data.values)

# GAN Parameters
input_dim = 3  # Income, Spending Score, Transactions
hidden_dim = 128
output_dim = 3
epochs = 3000  # More training epochs for better learning
batch_size = 128  # Larger batch size for better gradient estimation
learning_rate = 0.0002


# Step 3: Build the Generator and Discriminator

# Generator Network
class Generator(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)


# Discriminator Network
class Discriminator(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)


# Initialize Generator and Discriminator
generator = Generator(input_dim, hidden_dim, output_dim)
discriminator = Discriminator(input_dim, hidden_dim)

# Loss function and optimizers
criterion = nn.BCELoss()
optimizer_gen = optim.Adam(generator.parameters(), lr=learning_rate)
optimizer_disc = optim.Adam(discriminator.parameters(), lr=learning_rate)

# Step 4: Train the GAN
for epoch in range(epochs):
    for i in range(0, len(real_data_tensor), batch_size):
        # Real data
        real_batch = real_data_tensor[i:i + batch_size]
        real_label = torch.ones((real_batch.size(0), 1))

        # Fake data (generated by the generator)
        noise = torch.randn(real_batch.size(0), input_dim)  # Random noise for generator input
        fake_batch = generator(noise)
        fake_label = torch.zeros((real_batch.size(0), 1))

        # Train the discriminator
        optimizer_disc.zero_grad()
        output_real = discriminator(real_batch)
        output_fake = discriminator(fake_batch.detach())
        loss_disc_real = criterion(output_real, real_label)
        loss_disc_fake = criterion(output_fake, fake_label)
        loss_disc = (loss_disc_real + loss_disc_fake) / 2
        loss_disc.backward()
        optimizer_disc.step()

        # Train the generator
        optimizer_gen.zero_grad()
        output_fake = discriminator(fake_batch)
        loss_gen = criterion(output_fake, real_label)
        loss_gen.backward()
        optimizer_gen.step()

    if epoch % 500 == 0:
        print(f'Epoch [{epoch}/{epochs}] | Loss D: {loss_disc.item()} | Loss G: {loss_gen.item()}')


# Step 5: Traditional Data Generation (simulating ChainSys methods)
def chainsys_data_augmentation(real_data, num_samples):
    # Apply rule-based transformation for generating synthetic data
    generated_data = []
    for i in range(num_samples):
        # Example rule: if Loyalty Tier is Gold, Income and Transactions tend to be higher
        if np.random.choice(real_data['Loyalty Tier']) == 3:  # Gold tier
            income = np.random.normal(real_data['Income'].mean() + 10, real_data['Income'].std())
            transactions = np.random.normal(real_data['Transactions'].mean() + 10, real_data['Transactions'].std())
        else:
            income = np.random.normal(real_data['Income'].mean(), real_data['Income'].std())
            transactions = np.random.normal(real_data['Transactions'].mean(), real_data['Transactions'].std())

        # Spending score logic: Customers with higher income tend to have higher spending scores
        spending_score = income / 1.5 + np.random.normal(0, 5)

        # Ensure data validity with domain-specific rules (no negative values)
        income = max(0, income)
        transactions = max(0, transactions)
        spending_score = max(0, min(spending_score, 100))  # Spending score should be between 0 and 100

        generated_data.append([income, spending_score, transactions])

    return np.array(generated_data)


# Generate traditional synthetic data with rule-based methods
chainsys_synthetic_data = chainsys_data_augmentation(realistic_data, 500)

# Step 6: Generate GAN-based synthetic data using the trained GAN
noise = torch.randn(500, input_dim)
gan_generated_data = generator(noise).detach().numpy()
gan_generated_data = gan_generated_data * realistic_data[features_to_normalize].std().values + realistic_data[
    features_to_normalize].mean().values

# Step 7: Visualization - Compare Real, Traditional (ChainSys), and GAN-generated data

# Plot Real Data
plt.scatter(realistic_data['Income'], realistic_data['Spending Score'], label='Real Data', color='blue', alpha=0.5)

# Plot Traditional ChainSys Synthetic Data
plt.scatter(chainsys_synthetic_data[:, 0], chainsys_synthetic_data[:, 1], label='Traditional ChainSys Data',
            color='green', alpha=0.5)

# Plot GAN-Generated Data
plt.scatter(gan_generated_data[:, 0], gan_generated_data[:, 1], label='GAN-Generated Data', color='red', alpha=0.5)

# Labels and Title
plt.xlabel('Income (in thousands)')
plt.ylabel('Spending Score')
plt.legend()
plt.title('Comparison of Real, Traditional (ChainSys), and GAN-Generated Data')
plt.show()
